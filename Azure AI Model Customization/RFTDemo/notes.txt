Use Cases :https://platform.openai.com/docs/guides/rft-use-cases?chipstack=review&runloop=review&milo=use-case&ambience=review&harvey=review&accordance=review&safetykit=review&thomsonreuters=review#production-ready-api-snippets-that-compile-and-pass-ast-checks


Notes
1. RFT Usecases are 
    - Turning Instructions into working code
    - Pull facts into a clean format
    - Apply Complex rules correctly
2. Most of OpenAI usecases use Code Graders
3. Start with Eval. Ensure your base model is not always at min or max of the eval before rft
4. Demo should have an sturctured output, and then you can write complex evals to actually grade parts of the structured output
   differently.
5. You need training & test dataset for finetuning
6. Need to check with aoai.
    - Seems like we have an automated screening for dataset for oai. do we have this for aoai?
    - limit on max examples: 50K and test: 1K
    - Recomendation is to start quite small (few dozens to hundred and then increase. when increase increase batch size too)
    - screening dosent start on upload, it starts on first finetuning job
7. Try out the pydantic model syntax in code with aoai too
8. differentiated pricing for "share with openai", i assume we dont have this
9. Look at the reward metris over time for each grader in the UI.  A lot of information is given in the api, see if you can 
   pull up the metrics in the notebook and create more charts
10. See if our UI shows the Reasoning Token Mean chart and how it changes over time
11. See if we have the Grading Token Usage. also in the demo may be use 4o as judge and not o3


